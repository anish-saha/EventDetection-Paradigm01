{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "\n",
    "from pandas import DataFrame\n",
    "from datetime import datetime,tzinfo\n",
    "from pytz import timezone\n",
    "import time\n",
    "import pytz\n",
    "import csv\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>market</th>\n",
       "      <th>close_ratio</th>\n",
       "      <th>spread</th>\n",
       "      <th>7 Day Rolling Mean</th>\n",
       "      <th>7 Day SD</th>\n",
       "      <th>7 Day Volatility</th>\n",
       "      <th>isEventBaseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>763.28</td>\n",
       "      <td>777.51</td>\n",
       "      <td>713.60</td>\n",
       "      <td>735.07</td>\n",
       "      <td>46862700.0</td>\n",
       "      <td>8.955395e+09</td>\n",
       "      <td>0.3359</td>\n",
       "      <td>63.91</td>\n",
       "      <td>755.982857</td>\n",
       "      <td>24.982272</td>\n",
       "      <td>66.096878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>737.98</td>\n",
       "      <td>747.06</td>\n",
       "      <td>705.35</td>\n",
       "      <td>727.83</td>\n",
       "      <td>32505800.0</td>\n",
       "      <td>8.869919e+09</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>41.71</td>\n",
       "      <td>755.982857</td>\n",
       "      <td>24.982272</td>\n",
       "      <td>66.096878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>728.05</td>\n",
       "      <td>748.61</td>\n",
       "      <td>714.44</td>\n",
       "      <td>745.05</td>\n",
       "      <td>19011300.0</td>\n",
       "      <td>9.082104e+09</td>\n",
       "      <td>0.8958</td>\n",
       "      <td>34.17</td>\n",
       "      <td>755.982857</td>\n",
       "      <td>24.982272</td>\n",
       "      <td>66.096878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>741.35</td>\n",
       "      <td>766.60</td>\n",
       "      <td>740.24</td>\n",
       "      <td>756.13</td>\n",
       "      <td>20707700.0</td>\n",
       "      <td>9.217168e+09</td>\n",
       "      <td>0.6028</td>\n",
       "      <td>26.36</td>\n",
       "      <td>755.982857</td>\n",
       "      <td>24.982272</td>\n",
       "      <td>66.096878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>760.32</td>\n",
       "      <td>760.58</td>\n",
       "      <td>738.17</td>\n",
       "      <td>754.01</td>\n",
       "      <td>20897300.0</td>\n",
       "      <td>9.191325e+09</td>\n",
       "      <td>0.7068</td>\n",
       "      <td>22.41</td>\n",
       "      <td>755.982857</td>\n",
       "      <td>24.982272</td>\n",
       "      <td>66.096878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     open    high     low   close      volume        market  close_ratio  \\\n",
       "0  763.28  777.51  713.60  735.07  46862700.0  8.955395e+09       0.3359   \n",
       "1  737.98  747.06  705.35  727.83  32505800.0  8.869919e+09       0.5390   \n",
       "2  728.05  748.61  714.44  745.05  19011300.0  9.082104e+09       0.8958   \n",
       "3  741.35  766.60  740.24  756.13  20707700.0  9.217168e+09       0.6028   \n",
       "4  760.32  760.58  738.17  754.01  20897300.0  9.191325e+09       0.7068   \n",
       "\n",
       "   spread  7 Day Rolling Mean   7 Day SD  7 Day Volatility  isEventBaseline  \n",
       "0   63.91          755.982857  24.982272         66.096878                0  \n",
       "1   41.71          755.982857  24.982272         66.096878                0  \n",
       "2   34.17          755.982857  24.982272         66.096878                0  \n",
       "3   26.36          755.982857  24.982272         66.096878                0  \n",
       "4   22.41          755.982857  24.982272         66.096878                0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('baseline_def_kaggle.csv').iloc[:,1:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Model Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1349, 1349, 450, 450)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[df.columns[:-1]]\n",
    "y = df[df.columns[-1]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Performance:\n",
      "Training Accuracy: 0.9584877687175686\n",
      "Test Accuracy: 0.9533333333333334\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "mod1 = LogisticRegression()\n",
    "mod1.fit(X_train, y_train)\n",
    "\n",
    "print(\"Logistic Regression Model Performance:\")\n",
    "y_pred_train = mod1.predict(X_train)\n",
    "train_rmse = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Training Accuracy: \" + str(train_rmse))\n",
    "y_pred_test = mod1.predict(X_test)\n",
    "test_rmse = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Test Accuracy: \" + str(test_rmse))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# penalty = ['l1', 'l2']\n",
    "# C = np.logspace(0, 5, 10)\n",
    "# hyperparameters = dict(C=C, penalty=penalty)\n",
    "# clf = GridSearchCV(mod1, hyperparameters, cv=5, verbose=0)\n",
    "# mod2 = clf.fit(X_train, y_train) # optimized hyperparameters\n",
    "\n",
    "# print(\"Optimized Logistic Regression Model Performance:\")\n",
    "# y_pred_train = mod2.predict(X_train)\n",
    "# train_rmse = accuracy_score(y_train, y_pred_train)\n",
    "# print(\"Training Accuracy: \" + str(train_rmse))\n",
    "# y_pred_test = mod2.predict(X_test)\n",
    "# test_rmse = accuracy_score(y_test, y_pred_test)\n",
    "# print(\"Test Accuracy: \" + str(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Model Performance:\n",
      "Training Accuracy: 0.9621942179392142\n",
      "Test Accuracy: 0.9555555555555556\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Multi-Layer Perceptron Model Performance:\n",
      "Training Accuracy: 0.9621942179392142\n",
      "Test Accuracy: 0.9555555555555556\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Perceptron\n",
    "mod3 = Perceptron()\n",
    "mod3.fit(X_train, y_train)\n",
    "print(\"Perceptron Model Performance:\")\n",
    "y_pred_train = mod3.predict(X_train)\n",
    "train_rmse = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Training Accuracy: \" + str(train_rmse))\n",
    "y_pred_test = mod3.predict(X_test)\n",
    "test_rmse = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Test Accuracy: \" + str(test_rmse))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# hyperparameters = { \n",
    "#     'alpha': [0.0001, 0.05], 'fit_intercept': [True, False],\n",
    "#     'max_iter': [100, 1000], 'penalty': penalty \n",
    "# }\n",
    "# clf = GridSearchCV(mod3, hyperparameters, cv=5, verbose=0)\n",
    "# mod4 = clf.fit(X_train, y_train) # optimized hyperparameters\n",
    "\n",
    "# print(\"Optimized Perceptron Model Performance:\")\n",
    "# y_pred_train = mod4.predict(X_train)\n",
    "# train_rmse = accuracy_score(y_train, y_pred_train)\n",
    "# print(\"Training Accuracy: \" + str(train_rmse))\n",
    "# y_pred_test = mod4.predict(X_test)\n",
    "# test_rmse = accuracy_score(y_test, y_pred_test)\n",
    "# print(\"Test Accuracy: \" + str(test_rmse))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Multi-Layer Perceptron\n",
    "mod3 = MLPClassifier()\n",
    "mod3.fit(X_train, y_train)\n",
    "print(\"Multi-Layer Perceptron Model Performance:\")\n",
    "y_pred_train = mod3.predict(X_train)\n",
    "train_rmse = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Training Accuracy: \" + str(train_rmse))\n",
    "y_pred_test = mod3.predict(X_test)\n",
    "test_rmse = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Test Accuracy: \" + str(test_rmse))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# hyperparameters = { \n",
    "#     'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "#     'alpha': [0.0001, 0.05], 'activation': ['tanh', 'relu'],\n",
    "#     'learning_rate': ['constant','adaptive'] }\n",
    "# clf = GridSearchCV(mod3, hyperparameters, cv=5, verbose=0)\n",
    "# mod4 = clf.fit(X_train, y_train) # optimized hyperparameters\n",
    "\n",
    "# print(\"Optimized Multi-Layer Perceptron Model Performance:\")\n",
    "# y_pred_train = mod4.predict(X_train)\n",
    "# train_rmse = accuracy_score(y_train, y_pred_train)\n",
    "# print(\"Training Accuracy: \" + str(train_rmse))\n",
    "# y_pred_test = mod4.predict(X_test)\n",
    "\n",
    "# test_rmse = accuracy_score(y_test, y_pred_test)\n",
    "# print(\"Test Accuracy: \" + str(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Performance:\n",
      "Training Accuracy: 0.9903632320237212\n",
      "Test Accuracy: 0.9555555555555556\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "mod5 = RandomForestClassifier()\n",
    "mod5.fit(X_train, y_train)\n",
    "print(\"Random Forest Model Performance:\")\n",
    "y_pred_train = mod5.predict(X_train)\n",
    "train_rmse = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Training Accuracy: \" + str(train_rmse))\n",
    "y_pred_test = mod5.predict(X_test)\n",
    "test_rmse = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Test Accuracy: \" + str(test_rmse))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\n",
    "# max_depth.append(None)\n",
    "# hyperparameters = { \n",
    "#     'max_depth': max_depth, 'min_samples_split': [2, 5, 10],\n",
    "#     'max_features': ['auto', 'sqrt'], 'bootstrap': [True, False] \n",
    "# }\n",
    "# clf = GridSearchCV(mod5, hyperparameters, cv=5, verbose=0)\n",
    "# mod6 = clf.fit(X_train, y_train) # optimized hyperparameters\n",
    "\n",
    "# print(\"Optimized Random Forest Model Performance:\")\n",
    "# y_pred_train = mod6.predict(X_train)\n",
    "# train_rmse = accuracy_score(y_train, y_pred_train)\n",
    "# print(\"Training Accuracy: \" + str(train_rmse))\n",
    "# y_pred_test = mod6.predict(X_test)\n",
    "# test_rmse = accuracy_score(y_test, y_pred_test)\n",
    "# print(\"Test Accuracy: \" + str(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "It seems that a standard array of machine learning algorithms output relatively high accuracies. This is a good sign, but it is also important to note the findings in the EDA notebook –– these results may be extremely prone to bias, and our baseline definition may need to altered. However, much of the work in this notebook can be run verbatim with a different input file, so this should not prove to be much of an issue. With future iterations, we will be implementing optimization for these algorithms using the GridSearchCV library. If time permits, we may also implement a Recurrent Neural Network (RNN) solution and/or a timeseries-based analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
